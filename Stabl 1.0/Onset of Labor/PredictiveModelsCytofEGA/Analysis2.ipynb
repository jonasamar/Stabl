{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import asgl\n",
    "from pathlib import Path\n",
    "from sklearn import clone\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression, LassoCV, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold, LeaveOneOut, LeaveOneGroupOut\n",
    "from sklearn.svm import l1_min_c\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from stabl.pipelines_utils import compute_scores_table, save_plots\n",
    "from stabl.preprocessing import LowInfoFilter\n",
    "from stabl.metrics import jaccard_matrix\n",
    "\n",
    "from stabl.stacked_generalization import stacked_multi_omic\n",
    "import random\n",
    "\n",
    "lasso_cv = LassoCV(alphas=[10.**i for i in np.arange(-3, 1.1, 0.05)], max_iter=int(1e6), n_jobs=-1)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "preprocessing = Pipeline(\n",
    "    steps=[\n",
    "        (\"variance\", VarianceThreshold(0.)),\n",
    "        (\"lif\", LowInfoFilter()),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"std\", StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "X_ga = pd.read_csv('./Data/All/GA.csv',index_col=0)\n",
    "X_cytof = pd.read_csv('./Data/All/Cytof.csv',index_col=0)\n",
    "X_prot = pd.read_csv('./Data/All/Proteomics.csv',index_col=0)\n",
    "\n",
    "X_ga_term = pd.read_csv('./Data/Term/GA.csv',index_col=0)\n",
    "X_cytof_term = pd.read_csv('./Data/Term/Cytof.csv',index_col=0)\n",
    "X_prot_term = pd.read_csv('./Data/Term/Proteomics.csv',index_col=0)\n",
    "\n",
    "X_ga_preterm = pd.read_csv('./Data/Preterm/GA.csv',index_col=0)\n",
    "X_cytof_preterm = pd.read_csv('./Data/Preterm/Cytof.csv',index_col=0)\n",
    "X_prot_preterm = pd.read_csv('./Data/Preterm/Proteomics.csv',index_col=0)\n",
    "\n",
    "all_data_dict = {\n",
    " \t\t'CYTOF': X_cytof,\n",
    " \t\t'GA': X_ga,\n",
    " \t\t'Proteomics': X_prot\n",
    " \t}\n",
    "\n",
    "X_tot = pd.concat(all_data_dict.values(), axis=1)\n",
    "\n",
    "groups = pd.read_csv('./Data/All/ID.csv',index_col=0).iloc[:,0]\n",
    "y = pd.read_csv('./Data/All/DOS.csv',index_col=0).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CYTOF' 'GA' 'Proteomics']\n",
      "[0.0326053  0.53119046 0.43620424]\n",
      "['CYTOF' 'Proteomics']\n",
      "[0.06222248 0.93777752]\n",
      "Fold 1\n",
      "['CYTOF' 'GA' 'Proteomics']\n",
      "[0.56285096 0.2948851  0.14226393]\n",
      "['CYTOF' 'Proteomics']\n",
      "[0.7745862 0.2254138]\n",
      "Fold 2\n",
      "['CYTOF' 'GA' 'Proteomics']\n",
      "[0.04066454 0.03804535 0.92129011]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 119\u001b[0m\n\u001b[1;32m    110\u001b[0m   train_data_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mCYTOF\u001b[39m\u001b[39m'\u001b[39m: X_cytof\u001b[39m.\u001b[39miloc[train_index],\n\u001b[1;32m    112\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mProteomics\u001b[39m\u001b[39m'\u001b[39m: X_prot\u001b[39m.\u001b[39miloc[train_index]\n\u001b[1;32m    113\u001b[0m \t}\n\u001b[1;32m    114\u001b[0m   test_data_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mCYTOF\u001b[39m\u001b[39m'\u001b[39m: X_cytof\u001b[39m.\u001b[39miloc[test_index],\n\u001b[1;32m    116\u001b[0m \t\t\u001b[39m'\u001b[39m\u001b[39mProteomics\u001b[39m\u001b[39m'\u001b[39m: X_prot\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m    117\u001b[0m \t}\n\u001b[0;32m--> 119\u001b[0m   coefs, imp, preds \u001b[39m=\u001b[39m build_stack_model_and_predict(train_data_dict, y_train, test_data_dict)\n\u001b[1;32m    120\u001b[0m  \u001b[39m#  lf_predictions_dict[\"CyTOF&Proteomics\"] = pd.concat([lf_predictions_dict[\"CyTOF&Proteomics\"], pd.DataFrame({'sampleID': ids, 'pred' : preds}).set_index('sampleID')], axis=0)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m  \u001b[39m#  lf_coefs[\"CyTOF&Proteomics\"] = pd.concat([lf_coefs[\"CyTOF&Proteomics\"], coefs.rename(columns={'coef':f\"Fold {i}\"})], axis=1) \u001b[39;00m\n\u001b[1;32m    122\u001b[0m  \u001b[39m#  lf_importances[\"CyTOF&Proteomics\"] = pd.concat([lf_importances[\"CyTOF&Proteomics\"], imp.rename(columns={'importance':f\"Fold {i}\"})], axis=1) \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[133], line 44\u001b[0m, in \u001b[0;36mbuild_stack_model_and_predict\u001b[0;34m(train_data_dict, y_train, test_data_dict)\u001b[0m\n\u001b[1;32m     33\u001b[0m train_data_dict[omic_name] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     34\u001b[0m           data\u001b[39m=\u001b[39mpreprocessing\u001b[39m.\u001b[39mfit_transform(X_train),\n\u001b[1;32m     35\u001b[0m           columns\u001b[39m=\u001b[39mpreprocessing\u001b[39m.\u001b[39mget_feature_names_out(),\n\u001b[1;32m     36\u001b[0m           index\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mindex\n\u001b[1;32m     37\u001b[0m       )\n\u001b[1;32m     39\u001b[0m test_data_dict[omic_name] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     40\u001b[0m           data\u001b[39m=\u001b[39mpreprocessing\u001b[39m.\u001b[39mtransform(test_data_dict[omic_name]),\n\u001b[1;32m     41\u001b[0m           columns\u001b[39m=\u001b[39mpreprocessing\u001b[39m.\u001b[39mget_feature_names_out(),\n\u001b[1;32m     42\u001b[0m           index\u001b[39m=\u001b[39mtest_data_dict[omic_name]\u001b[39m.\u001b[39mindex\n\u001b[1;32m     43\u001b[0m       )\n\u001b[0;32m---> 44\u001b[0m fit_lasso \u001b[39m=\u001b[39m clone(lasso_cv)\u001b[39m.\u001b[39;49mfit(train_data_dict[omic_name], y_train)\n\u001b[1;32m     45\u001b[0m train_preds[omic_name] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(fit_lasso\u001b[39m.\u001b[39mpredict(train_data_dict[omic_name]))\n\u001b[1;32m     46\u001b[0m train_preds[omic_name]\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [omic_name]\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:1663\u001b[0m, in \u001b[0;36mLinearModelCV.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[39m# We do a double for loop folded in one, in order to be able to\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[39m# iterate in parallel on l1_ratio and folds\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m jobs \u001b[39m=\u001b[39m (\n\u001b[1;32m   1646\u001b[0m     delayed(_path_residuals)(\n\u001b[1;32m   1647\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m folds\n\u001b[1;32m   1662\u001b[0m )\n\u001b[0;32m-> 1663\u001b[0m mse_paths \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m   1664\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m   1665\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1666\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1667\u001b[0m )(jobs)\n\u001b[1;32m   1668\u001b[0m mse_paths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(mse_paths, (n_l1_ratio, \u001b[39mlen\u001b[39m(folds), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   1669\u001b[0m \u001b[39m# The mean is computed over folds.\u001b[39;00m\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Stabl/.venv/lib/python3.9/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1698\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1699\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   1700\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "splitter = LeaveOneGroupOut()\n",
    "\n",
    "ef_predictions_dict = {\"GA\":pd.DataFrame(), \"CyTOF&Proteomics\":pd.DataFrame(), \"CyTOF&Proteomics&GA\":pd.DataFrame()}\n",
    "lf_predictions_dict = {\"GA\":pd.DataFrame(), \"CyTOF&Proteomics\":pd.DataFrame(), \"CyTOF&Proteomics&GA\":pd.DataFrame()}\n",
    "\n",
    "ef_coefs = {\"CyTOF&Proteomics\":pd.DataFrame(), \"CyTOF&Proteomics&GA\":pd.DataFrame()}\n",
    "lf_coefs = {\"CyTOF&Proteomics\":pd.DataFrame(), \"CyTOF&Proteomics&GA\":pd.DataFrame()}\n",
    "lf_importances = {\"CyTOF&Proteomics\":pd.DataFrame(), \"CyTOF&Proteomics&GA\":pd.DataFrame()}\n",
    "\n",
    "def build_lasso_and_predict(X_train, y_train, X_test):\n",
    "  X_train = pd.DataFrame(\n",
    "            data=preprocessing.fit_transform(X_train),\n",
    "            columns=preprocessing.get_feature_names_out(),\n",
    "            index=X_train.index\n",
    "        )\n",
    "\n",
    "  X_test = pd.DataFrame(\n",
    "            data=preprocessing.transform(X_test),\n",
    "            columns=preprocessing.get_feature_names_out(),\n",
    "            index=X_test.index\n",
    "        )\n",
    "  fit_lasso = clone(lasso_cv).fit(X_train, y_train)\n",
    "  coefs = pd.DataFrame({'feature' : fit_lasso.feature_names_in_, 'coef': fit_lasso.coef_}).set_index('feature')\n",
    "  return(coefs, fit_lasso.predict(X_test))\n",
    "\n",
    "def build_stack_model_and_predict(train_data_dict, y_train, test_data_dict):\n",
    "  train_preds = dict()\n",
    "  test_preds = dict()\n",
    "  features = list()\n",
    "  coefs = list()\n",
    "  \n",
    "  for omic_name, X_train in train_data_dict.items():\n",
    "    train_data_dict[omic_name] = pd.DataFrame(\n",
    "              data=preprocessing.fit_transform(X_train),\n",
    "              columns=preprocessing.get_feature_names_out(),\n",
    "              index=X_train.index\n",
    "          )\n",
    "\n",
    "    test_data_dict[omic_name] = pd.DataFrame(\n",
    "              data=preprocessing.transform(test_data_dict[omic_name]),\n",
    "              columns=preprocessing.get_feature_names_out(),\n",
    "              index=test_data_dict[omic_name].index\n",
    "          )\n",
    "    fit_lasso = clone(lasso_cv).fit(train_data_dict[omic_name], y_train)\n",
    "    train_preds[omic_name] = pd.DataFrame(fit_lasso.predict(train_data_dict[omic_name]))\n",
    "    test_preds[omic_name] = pd.DataFrame(fit_lasso.predict(test_data_dict[omic_name]))\n",
    "    features = features + fit_lasso.feature_names_in_.tolist()\n",
    "    coefs = coefs + fit_lasso.coef_.tolist()\n",
    "    \n",
    "  train_preds_df = pd.concat(train_preds.values(), axis=1)\n",
    "  test_preds_df = pd.concat(test_preds.values(), axis=1)\n",
    "  stack_model = clone(rf_regressor).fit(train_preds_df, y_train)\n",
    "  coef_df = pd.DataFrame({'feature':features, 'coef':coefs}).set_index('feature')\n",
    "  imp = pd.DataFrame({\"omic\": train_preds.keys(), \"importance\" : stack_model.feature_importances_}).set_index(\"omic\")\n",
    "\n",
    "  return(coef_df, imp, stack_model.predict(test_preds_df))\n",
    "  \n",
    "    \n",
    "  fit_lasso = clone(lasso_cv).fit(X_train, y_train)\n",
    "  coefs = pd.DataFrame({'feature' : fit_lasso.feature_names_in_, 'coef': fit_lasso.coef_}).set_index('feature')\n",
    "  return(coefs, fit_lasso.predict(X_test))\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(splitter.split(X_ga, y, groups)):\n",
    "   print(f\"Fold {i}\")\n",
    "   ids = groups[test_index].index.to_list()\n",
    "   y_train = y[train_index]\n",
    "   y_test = y[test_index]\n",
    "    \n",
    "   ## All data\n",
    "   # EF\n",
    "   X_train = X_tot.iloc[train_index]\n",
    "   X_test = X_tot.iloc[test_index]\n",
    "   \n",
    "   coefs, preds = build_lasso_and_predict(X_train, y_train, X_test)\n",
    "   ef_predictions_dict[\"CyTOF&Proteomics&GA\"] = pd.concat([ef_predictions_dict[\"CyTOF&Proteomics&GA\"], pd.DataFrame({'sampleID': ids, 'pred' : preds}).set_index('sampleID')], axis=0)\n",
    "   ef_coefs[\"CyTOF&Proteomics&GA\"] = pd.concat([ef_coefs[\"CyTOF&Proteomics&GA\"], coefs.rename(columns={'coef':f\"Fold {i}\"})], axis=1)\n",
    "\n",
    "   # LF\n",
    "   train_data_dict = {\n",
    " \t  'CYTOF': X_cytof.iloc[train_index],\n",
    " \t\t'GA': X_ga.iloc[train_index],\n",
    " \t\t'Proteomics': X_prot.iloc[train_index]\n",
    " \t }\n",
    "   test_data_dict = {\n",
    " \t\t'CYTOF': X_cytof.iloc[test_index],\n",
    " \t\t'GA': X_ga.iloc[test_index],\n",
    " \t\t'Proteomics': X_prot.iloc[test_index]\n",
    " \t }\n",
    "   \n",
    "   coefs, imp, preds = build_stack_model_and_predict(train_data_dict, y_train, test_data_dict)\n",
    "   lf_predictions_dict[\"CyTOF&Proteomics&GA\"] = pd.concat([lf_predictions_dict[\"CyTOF&Proteomics&GA\"], pd.DataFrame({'sampleID': ids, 'pred' : preds}).set_index('sampleID')], axis=0)\n",
    "   lf_coefs[\"CyTOF&Proteomics&GA\"] = pd.concat([lf_coefs[\"CyTOF&Proteomics&GA\"], coefs.rename(columns={'coef':f\"Fold {i}\"})], axis=1) \n",
    "   lf_importances[\"CyTOF&Proteomics&GA\"] = pd.concat([lf_importances[\"CyTOF&Proteomics&GA\"], imp.rename(columns={'importance':f\"Fold {i}\"})], axis=1) \n",
    "\n",
    "   ## Prot + Cytof\n",
    "   # EF\n",
    "   X_train = pd.concat([all_data_dict[\"CYTOF\"], all_data_dict[\"Proteomics\"]], axis=1).iloc[train_index]\n",
    "   X_test = pd.concat([all_data_dict[\"CYTOF\"], all_data_dict[\"Proteomics\"]], axis=1).iloc[test_index]\n",
    "   \n",
    "   coefs, preds = build_lasso_and_predict(X_train, y_train, X_test)\n",
    "   ef_predictions_dict[\"CyTOF&Proteomics\"] = pd.concat([ef_predictions_dict[\"CyTOF&Proteomics\"], pd.DataFrame({'sampleID': ids, 'pred' : preds}).set_index('sampleID')], axis=0)\n",
    "   ef_coefs[\"CyTOF&Proteomics\"] = pd.concat([ef_coefs[\"CyTOF&Proteomics\"], coefs.rename(columns={'coef':f\"Fold {i}\"})], axis=1)\n",
    "   \n",
    "   # LF\n",
    "   train_data_dict = {\n",
    " \t\t'CYTOF': X_cytof.iloc[train_index],\n",
    " \t\t'Proteomics': X_prot.iloc[train_index]\n",
    " \t}\n",
    "   test_data_dict = {\n",
    " \t\t'CYTOF': X_cytof.iloc[test_index],\n",
    " \t\t'Proteomics': X_prot.iloc[test_index]\n",
    " \t}\n",
    "  \n",
    "   coefs, imp, preds = build_stack_model_and_predict(train_data_dict, y_train, test_data_dict)\n",
    "   lf_predictions_dict[\"CyTOF&Proteomics\"] = pd.concat([lf_predictions_dict[\"CyTOF&Proteomics\"], pd.DataFrame({'sampleID': ids, 'pred' : preds}).set_index('sampleID')], axis=0)\n",
    "   lf_coefs[\"CyTOF&Proteomics\"] = pd.concat([lf_coefs[\"CyTOF&Proteomics\"], coefs.rename(columns={'coef':f\"Fold {i}\"})], axis=1) \n",
    "   lf_importances[\"CyTOF&Proteomics\"] = pd.concat([lf_importances[\"CyTOF&Proteomics\"], imp.rename(columns={'importance':f\"Fold {i}\"})], axis=1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_predictions_dict[\"GA\"] = 7*(X_ga - 40)\n",
    "lf_predictions_dict[\"GA\"] = 7*(X_ga - 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"./Results2\"\n",
    "try:\n",
    "    os.mkdir(result_path)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_df = pd.concat(ef_predictions_dict.values(), axis=1)\n",
    "all_preds_df.columns = list(ef_predictions_dict.keys())\n",
    "all_preds_df.rename(columns={'CyTOF&Proteomics' : 'EF CyTOF&Proteomics', 'CyTOF&Proteomics&GA' : 'EF CyTOF&Proteomics&GA'}, inplace=True)\n",
    "all_preds_df = pd.concat([all_preds_df, lf_predictions_dict['CyTOF&Proteomics'], lf_predictions_dict['CyTOF&Proteomics&GA']], axis=1)\n",
    "all_preds_df.columns = list(all_preds_df.columns[:-2]) + ['LF CyTOF&Proteomics', 'LF CyTOF&Proteomics&GA']\n",
    "all_preds_df.to_csv(Path(result_path, 'predictions.csv'))\n",
    "\n",
    "y.to_csv(Path(result_path, 'outcomes.csv'))\n",
    "\n",
    "for key, el in ef_coefs.items():\n",
    "    el.to_csv(Path(result_path, f\"EF {key} coefficients.csv\"))\n",
    "\n",
    "for key, el in lf_coefs.items():\n",
    "    el.to_csv(Path(result_path, f\"LF {key} coefficients.csv\"))\n",
    "    \n",
    "for key, el in lf_importances.items():\n",
    "    el.to_csv(Path(result_path, f\"LF {key} omic importances.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_df = pd.read_csv(Path(result_path, 'predictions.csv'), index_col=0)\n",
    "y = pd.read_csv(Path(result_path, 'outcomes.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(index = all_preds_df.columns, columns = ['rmse', 'R2', 'Spearmanr', 'pvalue'])\n",
    "\n",
    "for col in all_preds_df.columns:\n",
    "    y_preds = all_preds_df[col]\n",
    "    lin_reg = clone(linreg).fit(np.array(y).reshape(-1, 1), np.array(y_preds).reshape(-1, 1))\n",
    "    y_reg = lin_reg.intercept_ + lin_reg.coef_[0]*y\n",
    "    scores.loc[col, 'R2'] = r2_score(y_preds, y_reg)\n",
    "    corr, pval = spearmanr(y_preds, y)\n",
    "    scores.loc[col, 'Spearmanr'], scores.loc[col, 'pvalue'] = corr, f'{pval:.6e}'\n",
    "    scores.loc[col, 'rmse'] = np.sqrt(mean_squared_error(y_preds, y))\n",
    "\n",
    "scores.to_csv(Path(result_path, 'scores.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
